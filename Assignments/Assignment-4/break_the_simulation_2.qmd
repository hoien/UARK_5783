---
title: "Break the Simulation 1"
author: "Your Name"
format: pdf
---
```{r setup}
#| include: false
library(fixest)
library(assertthat)
library(ggplot2)
library(purrr)
library(tinytable)
```

# Break the Simulation #2

## Original Simulation

```{r estimators}
# regression adjustment estimator
estimate_regadj <- function(df) { 
  est_y0 <- feols(
    y ~ 1 + x,
    data = subset(df, treat == 0)
  )
  est_y1 <- feols(
    y ~ 1 + x,
    data = subset(df, treat == 1)
  )
  y0_hat <- predict(est_y0, newdata = df)
  y1_hat <- predict(est_y1, newdata = df)
  tau_hat <- mean(y1_hat) - mean(y0_hat)
  return(tau_hat)
}

# iptw estimator
estimate_iptw <- function(df, type = "hajek") {
  est_propensity <- feglm(
    treat ~ 1 + x,
    data = df, family = "logit"
  )
  pi_hat <- predict(est_propensity, newdata = df)
  D <- df$treat
  y <- df$y
  w_1 <- D / pi_hat
  w_0 <- (1 - D) / (1 - pi_hat)

  if (type == "hajek") {
    w_1 <- w_1 / mean(w_1)
    w_0 <- w_0 / mean(w_0)
  }

  tau_hat <- mean(w_1 * y) - mean(w_0 * y)
  return(tau_hat)
}

# doubly-robust estimator
estimate_aiptw <- function(df, type = "hajek") {
  est_y0 <- feols(
    y ~ 1 + x,
    data = subset(df, treat == 0)
  )
  est_y1 <- feols(
    y ~ 1 + x,
    data = subset(df, treat == 1)
  )
  est_propensity <- feglm(
    treat ~ 1 + x,
    data = df, family = "logit"
  )
  
  y0_hat <- predict(est_y0, newdata = df)
  y1_hat <- predict(est_y1, newdata = df)
  pi_hat <- predict(est_propensity, newdata = df)
  D <- df$treat
  y <- df$y
  
  w_1 <- D / pi_hat
  w_0 <- (1 - D) / (1 - pi_hat)

  if (type == "hajek") {
    w_1 <- w_1 / mean(w_1)
    w_0 <- w_0 / mean(w_0)
  }

  tau_hat <- (mean(y1_hat) - mean(y0_hat)) + 
    (mean((w_1 * (y - y1_hat))[D == 1]) - mean((w_0 * (y - y0_hat))[D == 0]))
  return(tau_hat)
}

```

```{r}
run_monte_carlo <- function(dgp) {
  purrr::list_rbind(purrr::map(1:500, function(i) {
    df <- dgp()

    true_te <- mean(df$y1 - df$y0)
    
    tau_hat_ra <- estimate_regadj(df)
    tau_hat_iptw <- estimate_iptw(df)
    tau_hat_aiptw <- estimate_aiptw(df)
  
    estimates <- data.frame(
      iter = i,
      true_te = true_te, 
      tau_hat_ra = tau_hat_ra, 
      tau_hat_iptw = tau_hat_iptw, 
      tau_hat_aiptw = tau_hat_aiptw
    )

    return(estimates)
  }))
}

summarize_mc_results <- function(mc) {
  res <- data.frame(
    x = c("True ATE", "Avg. Regression Adjustment", "Avg. IPTW", "Avg. AIPTW"), 
    y = c(mean(mc$true_te), mean(mc$tau_hat_ra), mean(mc$tau_hat_iptw), mean(mc$tau_hat_aiptw))
  )

  tinytable::tt(res) |> 
    print("markdown")
} 

plot_mc_results <- function(mc) {
  ggplot(data = mc) + 
    geom_density(
      aes(x = tau_hat_ra, color = "Regression Adjustment"),
      linewidth = 1.5, key_glyph = "path"
    ) +
    geom_density(
      aes(x = tau_hat_iptw, color = "IPTW"),
      linewidth = 1.5, key_glyph = "path"
    ) +
    geom_density(
      aes(x = tau_hat_aiptw, color = "AIPTW"),
      linewidth = 1.5, key_glyph = "path"
    ) +
    scale_color_manual(
      values = c("#B3114B", "#2DB25F", "#ffc517", "#5C4CBF"),
      guide = guide_legend()
    ) + 
    geom_vline(xintercept = mean(mc$true_te), linetype = "dashed") + 
    labs(
      x = "Difference-in-means Estimate", 
      y = "Count", 
      color = NULL
    ) +
    theme_bw(base_size = 12) + 
    theme(
      legend.position = "top",
      legend.text = element_text(size = rel(1.25)),
      legend.key.spacing.x = unit(24, "pt")
    )
}
```

## Simulation using clean dgp

```{r}
# This is the original code and should not be changed
dgp_clean <- function() {  
  n <- 500
  x <- runif(n = n, -1, 1)

  # generate propensity score (following logit equation)
  pi <- 1 / (1 + exp(0 - 0.6 * x))

  # generate treatment according to propensity score
  d <- as.numeric(runif(n = n, 0, 1) < pi)

  # generate observed Y (following linear model)
  u <- rnorm(n = n, mean = 0, sd = 0.4)
  y0 <- 2 * x + u
  y1 <- 1 + 2 * x + u  
  y <- d * y1 + (1 - d) * y0

  # return as data.frame
  df <- data.frame(
    x = x, treat = d, y = y, y1 = y1, y0 = y0
  )
  return(df)
}
```

```{r mc_dgp_clean}
set.seed(20241002)
mc_dgp_clean <- run_monte_carlo(dgp_clean)
summarize_mc_results(mc_dgp_clean)
plot_mc_results(mc_dgp_clean)
```


## Question 1

For this first question, I want you to break all the estimators (note the DIM estimator is already biased). Hint: think about the conditional independence assumption

```{r}
# This is the *only* code block that you should modify!
dgp_broken_q1 <- function(true_te = 1) {
  n <- 500
  x <- runif(n = n, -1, 1)

  # generate propensity score (following logit equation)
  pi <- 1 / (1 + exp(0 - 0.6 * x))

  # generate treatment according to propensity score
  d <- as.numeric(runif(n = n, 0, 1) < pi)

  # generate observed Y (following linear model)
  u <- rnorm(n = n, mean = 0, sd = 0.4)
  y0 <- 2 * x + u
  y1 <- 1 + 2 * x + u  
  y <- d * y1 + (1 - d) * y0

  # return as data.frame
  df <- data.frame(
    x = x, treat = d, y = y, y1 = y1, y0 = y0
  )
  return(df)
}
```

```{r}
set.seed(20241002)
mc_dgp_broken_q1 <- run_monte_carlo(dgp_broken_q1)
summarize_mc_results(mc_dgp_broken_q1)
plot_mc_results(mc_dgp_broken_q1)
```

### Describe your thinking

I did ... because ...


## Question 2

This question is going to be a little more tricky. What I want you to do is break *only* the regression adjustment estimator without breaking the IPTW estimator

```{r}
# This is the *only* code block that you should modify!
dgp_broken_q2 <- function() {
  n <- 500
  x <- runif(n = n, -1, 1)

  # generate propensity score (following logit equation)
  pi <- 1 / (1 + exp(0 - 0.6 * x))

  # generate treatment according to propensity score
  d <- as.numeric(runif(n = n, 0, 1) < pi)

  # generate observed Y (following linear model)
  u <- rnorm(n = n, mean = 0, sd = 0.4)
  y0 <- 2 * x + u
  y1 <- 1 + 2 * x + 3 * x^3 + u  
  y <- d * y1 + (1 - d) * y0

  # return as data.frame
  df <- data.frame(
    x = x, treat = d, y = y, y1 = y1, y0 = y0
  )
  return(df)
}
```

```{r}
set.seed(20241002)
mc_dgp_broken_q2 <- run_monte_carlo(dgp_broken_q2)
summarize_mc_results(mc_dgp_broken_q2)
plot_mc_results(mc_dgp_broken_q2)
```

### Describe your thinking

I did ... because ...


